{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfbb26db-26e6-4002-a9d2-e5f57fc6e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "import copy\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd16a5d9-48ba-4cd7-9000-4f3505110ad5",
   "metadata": {},
   "source": [
    "# Loading Word2Vec Model\n",
    "Loading Word2vec pretrained model on google news 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "031e35fd-9f6d-48b7-b3b8-561dafa8f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec embeddings \n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6213a3-71fa-44d2-a655-a9d20a886071",
   "metadata": {},
   "source": [
    "# Loading Glove Model \n",
    "Loading Glove model with 42B word vectors and 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4d587e-c845-40ec-bba3-e0ff2522cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file_path):\n",
    "    # Convert GloVe to Word2Vec format directly in Gensim\n",
    "    model = KeyedVectors.load_word2vec_format(glove_file_path, no_header=True, binary=False)\n",
    "    return model\n",
    "\n",
    "glove_file_path = \"/mnt/hdd/karmpatel/naman/demo/glove/glove.42B.300d.txt\"\n",
    "glove_model = load_glove_model(glove_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf559c-4b7c-4516-8d60-6c5feb5a7a64",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "1. **WordSim353**\n",
    "2. **RG65**\n",
    "3. **Men3000**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3606f5a6-79a4-426c-a28a-606c755ea7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame shape : (353, 4)\n",
      "Dataframe top 5 rows\n",
      "<bound method NDFrame.head of             word1     word2  similarity_score  similarity_score_normalized\n",
      "0            love       sex              6.77                     0.420200\n",
      "1           tiger       cat              7.35                     0.686808\n",
      "2           tiger     tiger             10.00                     1.904931\n",
      "3            book     paper              7.46                     0.737372\n",
      "4        computer  keyboard              7.62                     0.810919\n",
      "..            ...       ...               ...                          ...\n",
      "348        shower     flood              6.03                     0.080045\n",
      "349       weather  forecast              8.34                     1.141880\n",
      "350      disaster      area              6.25                     0.181172\n",
      "351      governor    office              6.34                     0.222542\n",
      "352  architecture   century              3.78                    -0.954210\n",
      "\n",
      "[353 rows x 4 columns]>\n",
      "Unique keys : 437\n"
     ]
    }
   ],
   "source": [
    "file_names = [\"wordsim353.csv\", \"rg65.csv\", \"men3000.csv\"]\n",
    "file_path = f\"/mnt/hdd/karmpatel/naman/demo/all_but_the_top/{file_names[0]}\"\n",
    "df = pd.read_csv(file_path, header=None, names=['word1', 'word2', 'similarity_score'], delimiter=';')\n",
    "\n",
    "# Normalizing score\n",
    "mean = df['similarity_score'].mean()\n",
    "std = df['similarity_score'].std()\n",
    "df['similarity_score_normalized'] = (df['similarity_score'] - mean) / std\n",
    "\n",
    "print(f\"Data frame shape : {df.shape}\")\n",
    "print(\"Dataframe top 5 rows\")\n",
    "print(df.head)\n",
    "\n",
    "# Get unique words\n",
    "unique_words = pd.concat([df['word1'], df['word2']]).unique()\n",
    "unique_words_ls = unique_words.tolist()\n",
    "# Sort all the unique words \n",
    "unique_words_ls.sort() \n",
    "\n",
    "# Create unique word mapping \n",
    "# Key : Word , Value : Index \n",
    "unique_words_mapper_word2idx = {} \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    unique_words_mapper_word2idx[unique_words_ls[i]] = i \n",
    "\n",
    "print(f\"Unique keys : {len(unique_words_mapper_word2idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2619e1-5b95-4e2c-8cdc-c9f6c00c5f28",
   "metadata": {},
   "source": [
    "# Creating embedding matrix \n",
    "1. Word2Vec\n",
    "2. Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87669185-ef47-4736-9b27-a4ca4ed05e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec matrix shape : (437, 300)\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec Matrix\n",
    "word2vec_matrix = [] \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    word2vec_matrix.append(word2vec_model[unique_words_ls[i]])\n",
    "    \n",
    "word2vec_matrix = np.vstack(word2vec_matrix)\n",
    "print(f\"Word2vec matrix shape : {word2vec_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c775ac6-bfc5-496b-b6e5-c98a3a7f93cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove matrix shape : (437, 300)\n"
     ]
    }
   ],
   "source": [
    "# Glove Matrix \n",
    "glove_matrix = [] \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    word = unique_words_ls[i]\n",
    "    vector = glove_model.get_vector(word) if word in glove_model else None\n",
    "    if vector is not None : \n",
    "        glove_matrix.append(vector)\n",
    "    else : \n",
    "        glove_matrix.append(np.zeros(300))\n",
    "\n",
    "glove_matrix = np.vstack(glove_matrix)\n",
    "print(f\"Glove matrix shape : {glove_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c3e688-1d74-46cf-801f-3c260e1dccee",
   "metadata": {},
   "source": [
    "# Post processing algorithm \n",
    "1. Zero mean vector\n",
    "2. Subtracting top D dimension, D can vary from d = 1, 2, 3, ...\n",
    "\n",
    "In paper they have mentioned that D is a hyperparameter, and for illustration purpose we have fixed it to 1, where it is giving best performance, but generally it is around d/100 size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2821bb1a-62b5-4e23-8493-0463cf43f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_embeddings(embedding_matrix_orig, n_components = 1):\n",
    "\tpca = PCA(n_components=n_components)\n",
    "\tembedding_matrix = copy.deepcopy(embedding_matrix_orig)\n",
    "\tmean = np.average(embedding_matrix, axis=0)\n",
    "\ttemp = embedding_matrix - mean\n",
    "\tprincipalComponents = pca.fit_transform(temp)\n",
    "\tprincipalAxes = pca.components_\n",
    "\ttoSubstract = np.matmul(np.matmul(embedding_matrix, principalAxes.T), principalAxes)\n",
    "\tprocessed = temp - toSubstract\n",
    "\treturn processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "960571db-5d1d-40db-afc1-a85788308a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec matrix postprocess shape : (437, 300)\n",
      "Glove matrix postprocess shape : (437, 300)\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "word2vec_matrix_postprocessed = get_processed_embeddings(word2vec_matrix, n_components = 1)\n",
    "print(f\"Word2vec matrix postprocess shape : {word2vec_matrix_postprocessed.shape}\")\n",
    "\n",
    "# Glove\n",
    "glove_matrix_postprocessed = get_processed_embeddings(glove_matrix, n_components = 1)\n",
    "print(f\"Glove matrix postprocess shape : {glove_matrix_postprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "578f8ff5-d6ce-419a-9ee9-2acb6a25d169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>similarity_score_normalized</th>\n",
       "      <th>word2vec_cs_original</th>\n",
       "      <th>word2vec_cs_postprocess</th>\n",
       "      <th>glove_cs_original</th>\n",
       "      <th>glove_cs_postprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.420200</td>\n",
       "      <td>0.110907</td>\n",
       "      <td>0.061679</td>\n",
       "      <td>0.211496</td>\n",
       "      <td>0.163040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.686808</td>\n",
       "      <td>0.355283</td>\n",
       "      <td>0.261792</td>\n",
       "      <td>0.344203</td>\n",
       "      <td>0.312226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>1.904931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book</td>\n",
       "      <td>paper</td>\n",
       "      <td>7.46</td>\n",
       "      <td>0.737372</td>\n",
       "      <td>0.268007</td>\n",
       "      <td>0.227952</td>\n",
       "      <td>0.401583</td>\n",
       "      <td>0.302883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>computer</td>\n",
       "      <td>keyboard</td>\n",
       "      <td>7.62</td>\n",
       "      <td>0.810919</td>\n",
       "      <td>0.321441</td>\n",
       "      <td>0.279749</td>\n",
       "      <td>0.455347</td>\n",
       "      <td>0.451020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word1     word2  similarity_score  similarity_score_normalized  \\\n",
       "0      love       sex              6.77                     0.420200   \n",
       "1     tiger       cat              7.35                     0.686808   \n",
       "2     tiger     tiger             10.00                     1.904931   \n",
       "3      book     paper              7.46                     0.737372   \n",
       "4  computer  keyboard              7.62                     0.810919   \n",
       "\n",
       "   word2vec_cs_original  word2vec_cs_postprocess  glove_cs_original  \\\n",
       "0              0.110907                 0.061679           0.211496   \n",
       "1              0.355283                 0.261792           0.344203   \n",
       "2              1.904931                 1.904931           1.904931   \n",
       "3              0.268007                 0.227952           0.401583   \n",
       "4              0.321441                 0.279749           0.455347   \n",
       "\n",
       "   glove_cs_postprocess  \n",
       "0              0.163040  \n",
       "1              0.312226  \n",
       "2              1.904931  \n",
       "3              0.302883  \n",
       "4              0.451020  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine score of word2vec original \n",
    "df['word2vec_cs_original'] = df.apply(lambda row: (\n",
    "    row['similarity_score_normalized'] * (\n",
    "        np.dot(\n",
    "            word2vec_matrix[unique_words_mapper_word2idx[row['word1']]],\n",
    "            word2vec_matrix[unique_words_mapper_word2idx[row['word2']]]\n",
    "        ) / (\n",
    "            np.linalg.norm(word2vec_matrix[unique_words_mapper_word2idx[row['word1']]]) *\n",
    "            np.linalg.norm(word2vec_matrix[unique_words_mapper_word2idx[row['word2']]])\n",
    "        )\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "df['word2vec_cs_postprocess'] = df.apply(lambda row: (\n",
    "    row['similarity_score_normalized'] * (\n",
    "        np.dot(\n",
    "            word2vec_matrix_postprocessed[unique_words_mapper_word2idx[row['word1']]],\n",
    "            word2vec_matrix_postprocessed[unique_words_mapper_word2idx[row['word2']]]\n",
    "        ) / (\n",
    "            np.linalg.norm(word2vec_matrix_postprocessed[unique_words_mapper_word2idx[row['word1']]]) *\n",
    "            np.linalg.norm(word2vec_matrix_postprocessed[unique_words_mapper_word2idx[row['word2']]])\n",
    "        )\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "df['glove_cs_original'] = df.apply(lambda row: (\n",
    "    row['similarity_score_normalized'] * (\n",
    "        np.dot(\n",
    "            glove_matrix[unique_words_mapper_word2idx[row['word1']]],\n",
    "            glove_matrix[unique_words_mapper_word2idx[row['word2']]]\n",
    "        ) / (\n",
    "            np.linalg.norm(glove_matrix[unique_words_mapper_word2idx[row['word1']]]) *\n",
    "            np.linalg.norm(glove_matrix[unique_words_mapper_word2idx[row['word2']]])\n",
    "        )\n",
    "    ) if np.all(glove_matrix[unique_words_mapper_word2idx[row['word1']]] != 0) and np.all(glove_matrix[unique_words_mapper_word2idx[row['word2']]] != 0)\n",
    "    else 0\n",
    "), axis=1)\n",
    "\n",
    "\n",
    "df['glove_cs_postprocess'] = df.apply(lambda row: (\n",
    "    row['similarity_score_normalized'] * (\n",
    "        np.dot(\n",
    "            glove_matrix_postprocessed[unique_words_mapper_word2idx[row['word1']]],\n",
    "            glove_matrix_postprocessed[unique_words_mapper_word2idx[row['word2']]]\n",
    "        ) / (\n",
    "            np.linalg.norm(glove_matrix_postprocessed[unique_words_mapper_word2idx[row['word1']]]) *\n",
    "            np.linalg.norm(glove_matrix_postprocessed[unique_words_mapper_word2idx[row['word2']]])\n",
    "        )\n",
    "    )  if np.all(glove_matrix[unique_words_mapper_word2idx[row['word1']]] != 0) and np.all(glove_matrix[unique_words_mapper_word2idx[row['word2']]] != 0)\n",
    "    else 0\n",
    "), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66ccde55-22c7-4d1e-8ad8-55ed41ae8bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      " Word2vec Original :0.11486492291948834\n",
      " Word2vec Postprocess :0.12044304573534262\n",
      " Glove Original :0.0894694131641246\n",
      " Glove Postprocess :0.09754523810300944\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine Similarity\")\n",
    "print(f\" Word2vec Original :{df['word2vec_cs_original'].mean()}\") \n",
    "print(f\" Word2vec Postprocess :{df['word2vec_cs_postprocess'].mean()}\") \n",
    "print(f\" Glove Original :{df['glove_cs_original'].mean()}\") \n",
    "print(f\" Glove Postprocess :{df['glove_cs_postprocess'].mean()}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fc156-e42d-4e23-b6a4-f8cee0da5ef8",
   "metadata": {},
   "source": [
    "# Word Anology Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d93ab2bc-b487-4a0a-bef0-72c2593425f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_one</th>\n",
       "      <th>word_two</th>\n",
       "      <th>word_three</th>\n",
       "      <th>word_four</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Baghdad</td>\n",
       "      <td>Iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Athens</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Bern</td>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19539</th>\n",
       "      <td>write</td>\n",
       "      <td>writes</td>\n",
       "      <td>talk</td>\n",
       "      <td>talks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19540</th>\n",
       "      <td>write</td>\n",
       "      <td>writes</td>\n",
       "      <td>think</td>\n",
       "      <td>thinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19541</th>\n",
       "      <td>write</td>\n",
       "      <td>writes</td>\n",
       "      <td>vanish</td>\n",
       "      <td>vanishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19542</th>\n",
       "      <td>write</td>\n",
       "      <td>writes</td>\n",
       "      <td>walk</td>\n",
       "      <td>walks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19543</th>\n",
       "      <td>write</td>\n",
       "      <td>writes</td>\n",
       "      <td>work</td>\n",
       "      <td>works</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19544 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_one word_two word_three    word_four\n",
       "0       Athens   Greece    Baghdad         Iraq\n",
       "1       Athens   Greece    Bangkok     Thailand\n",
       "2       Athens   Greece    Beijing        China\n",
       "3       Athens   Greece     Berlin      Germany\n",
       "4       Athens   Greece       Bern  Switzerland\n",
       "...        ...      ...        ...          ...\n",
       "19539    write   writes       talk        talks\n",
       "19540    write   writes      think       thinks\n",
       "19541    write   writes     vanish     vanishes\n",
       "19542    write   writes       walk        walks\n",
       "19543    write   writes       work        works\n",
       "\n",
       "[19544 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = f\"/mnt/hdd/karmpatel/naman/demo/all_but_the_top/questions-words.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.drop(columns = ['row_id', 'category'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43ac9fa6-f3c0-4419-8260-2f634c2a0924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keys : 905\n"
     ]
    }
   ],
   "source": [
    "# Get unique words\n",
    "unique_words = pd.concat([df['word_one'], df['word_two'], df['word_three'], df['word_four']]).unique()\n",
    "unique_words_ls = unique_words.tolist()\n",
    "# Sort all the unique words \n",
    "unique_words_ls.sort() \n",
    "\n",
    "# Create unique word mapping \n",
    "# Key : Word , Value : Index \n",
    "unique_words_mapper_word2idx = {} \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    unique_words_mapper_word2idx[unique_words_ls[i]] = i \n",
    "\n",
    "print(f\"Unique keys : {len(unique_words_mapper_word2idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48e4bf45-a1e1-448f-82f0-1c0eab0e53cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec matrix shape : (905, 300)\n"
     ]
    }
   ],
   "source": [
    "# Compute Vector v(w2) - v(w1) + v(3)\n",
    "# Word2Vec Matrix\n",
    "word2vec_matrix = [] \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    word2vec_matrix.append(word2vec_model[unique_words_ls[i]])\n",
    "    \n",
    "word2vec_matrix = np.vstack(word2vec_matrix)\n",
    "print(f\"Word2vec matrix shape : {word2vec_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d800d1e-407e-4c0b-a574-6b29c96e7e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove matrix shape : (905, 300)\n"
     ]
    }
   ],
   "source": [
    "# Glove Matrix \n",
    "glove_matrix = [] \n",
    "for i in range(len(unique_words_ls)) : \n",
    "    word = unique_words_ls[i]\n",
    "    vector = glove_model.get_vector(word) if word in glove_model else None\n",
    "    if vector is not None : \n",
    "        glove_matrix.append(vector)\n",
    "    else : \n",
    "        glove_matrix.append(np.zeros(300))\n",
    "\n",
    "glove_matrix = np.vstack(glove_matrix)\n",
    "print(f\"Glove matrix shape : {glove_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7af9f964-88cb-447a-a4de-36735814ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec matrix postprocess shape : (905, 300)\n",
      "Glove matrix postprocess shape : (905, 300)\n"
     ]
    }
   ],
   "source": [
    "# Word2vec\n",
    "word2vec_matrix_postprocessed = get_processed_embeddings(word2vec_matrix, n_components = 1)\n",
    "print(f\"Word2vec matrix postprocess shape : {word2vec_matrix_postprocessed.shape}\")\n",
    "\n",
    "# Glove\n",
    "glove_matrix_postprocessed = get_processed_embeddings(glove_matrix, n_components = 1)\n",
    "print(f\"Glove matrix postprocess shape : {glove_matrix_postprocessed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fae6a-c971-43ba-80fc-b32f179d086c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
